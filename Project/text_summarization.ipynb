{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('anaconda3': virtualenv)",
   "display_name": "Python 3.7.7 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "ead85409a3df2736fa4852a03cf3afe7e5b1dbdc0e40d7d22bbcbf8cf1adf5fa"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalizing\n",
    "import re\n",
    "  \n",
    "kor_begin     = 44032\n",
    "kor_end       = 55203\n",
    "chosung_base  = 588\n",
    "jungsung_base = 28\n",
    "jaum_begin = 12593\n",
    "jaum_end = 12622\n",
    "moum_begin = 12623\n",
    "moum_end = 12643\n",
    "\n",
    "chosung_list = [ 'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', \n",
    "        'ㅅ', 'ㅆ', 'ㅇ' , 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "jungsung_list = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', \n",
    "        'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', \n",
    "        'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', \n",
    "        'ㅡ', 'ㅢ', 'ㅣ']\n",
    "\n",
    "jongsung_list = [\n",
    "    ' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ',\n",
    "        'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', \n",
    "        'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', \n",
    "        'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "jaum_list = ['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄸ', 'ㄹ', \n",
    "              'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', \n",
    "              'ㅃ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "moum_list = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', \n",
    "              'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "\n",
    "doublespace_pattern = re.compile('\\s+')\n",
    "repeatchars_pattern = re.compile('(\\w)\\\\1{3,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(doc, english=True, number=True, punctuation=False, remove_repeat = 0, remains={}):\n",
    "    if remove_repeat > 0:\n",
    "        doc = repeatchars_pattern.sub('\\\\1' * remove_repeat, doc)\n",
    "\n",
    "    f = ''    \n",
    "    for c in doc:\n",
    "        i = ord(c)\n",
    "        \n",
    "        if (c == ' ') or (is_korean(i)) or (english and is_english(i)) or (number and is_number(i)) or (punctuation and is_punctuation(i)):\n",
    "            f += c            \n",
    "        elif c in remains:\n",
    "            f += c        \n",
    "        else:\n",
    "            f += ' '\n",
    "            \n",
    "    return doublespace_pattern.sub(' ', f).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_jamo(c):    \n",
    "    i = ord(c)\n",
    "    \n",
    "    if not is_korean(i):\n",
    "        return None\n",
    "    elif is_jaum(i):\n",
    "        return [c, ' ', ' ']\n",
    "    elif is_moum(i):\n",
    "        return [' ', c, ' ']\n",
    "    \n",
    "    i -= kor_begin\n",
    "    \n",
    "    cho  = i // chosung_base\n",
    "    jung = ( i - cho * chosung_base ) // jungsung_base \n",
    "    jong = ( i - cho * chosung_base - jung * jungsung_base )\n",
    "    \n",
    "    return [chosung_list[cho], jungsung_list[jung], jongsung_list[jong]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_korean(i):\n",
    "    i = to_base(i)\n",
    "    return (kor_begin <= i <= kor_end) or (jaum_begin <= i <= jaum_end) or (moum_begin <= i <= moum_end)\n",
    "\n",
    "def is_number(i):\n",
    "    i = to_base(i)\n",
    "    return (i >= 48 and i <= 57)\n",
    "\n",
    "def is_english(i):\n",
    "    i = to_base(i)\n",
    "    return (i >= 97 and i <= 122) or (i >= 65 and i <= 90)\n",
    "\n",
    "def is_punctuation(i):\n",
    "    i = to_base(i)\n",
    "    return (i == 33 or i == 34 or i == 39 or i == 44 or i == 46 or i == 63 or i == 96)\n",
    "\n",
    "def is_jaum(i):\n",
    "    i = to_base(i)\n",
    "    return (jaum_begin <= i <= jaum_end)\n",
    "\n",
    "def is_moum(i):\n",
    "    i = to_base(i)\n",
    "    return (moum_begin <= i <= moum_end)\n",
    "\n",
    "def to_base(c):\n",
    "    if type(c) == str:\n",
    "        return ord(c)\n",
    "    elif type(c) == int:\n",
    "        return c\n",
    "    else:\n",
    "        raise TypeError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_jamo(chosung, jungsung, jongsung):\n",
    "    return chr(kor_begin + chosung_base * chosung_list.index(chosung) + jungsung_base * jungsung_list.index(jungsung) + jongsung_list.index(jongsung))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNN_Encoder:\n",
    "        \n",
    "    def __init__(self, vocabs={}):\n",
    "        self.vocabs = vocabs\n",
    "        \n",
    "        self.jungsung_hot_begin = 31\n",
    "        self.jongsung_hot_begin = 52\n",
    "        self.symbol_hot_begin = 83\n",
    "\n",
    "        self.cvocabs_ = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄸ', 'ㄹ', \n",
    "                   'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅃ', \n",
    "                   'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', \n",
    "                   'ㅎ', 'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ',\n",
    "                   'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', \n",
    "                   'ㅢ', 'ㅣ']\n",
    "        self.cvocabs = {}\n",
    "        self.cvocabs = {c:len(cvocabs) + 1 for c in cvocabs_}\n",
    "\n",
    "        # svocabs_ = ['.',  ',',  '?',  '!',  '-', ':', \n",
    "        #            '0',  '1',  '2',  '3',  '4',  '5',  '6',  '7',  '8',  '9']\n",
    "        # svocabs = {}\n",
    "        # svocabs = {s:len(svocabs) + symbol_hot_begin for s in svocabs_}\n",
    "\n",
    "\n",
    "    def encode_vocab(self, words, unknown=-1, blank=0, input_length=64):\n",
    "        if len(words) > input_length:\n",
    "            words = words[:input_length]\n",
    "        return [self.vocabs[w] if w in self.vocabs else unknown for w in words] + [blank] * (input_length - len(words))\n",
    "\n",
    "    def encode_jamo_onehot(self, chars, input_length=64, as_ndarray=False):\n",
    "        ints = []\n",
    "        return ints\n",
    "\n",
    "    def encode_jamo_threehot(self, chars, input_length=64, as_ndarray=False):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, reversed_dict, article_max_len, summary_max_len, embedding_size, num_hidden, num_layers, learning_rate, beam_width, keep_prob, glove, forward_only=False):\n",
    "        self.vocabulary_size = len(reversed_dict)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beam_width = beam_width\n",
    "        if not forward_only:\n",
    "            self.keep_prob = keep_prob\n",
    "        else:\n",
    "            self.keep_prob = 1.0\n",
    "        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        with tf.variable_scope(\"decoder/projection\"):\n",
    "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
    "        self.X_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            if not forward_only and glove:\n",
    "                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)\n",
    "            else:\n",
    "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
    "            self.embeddings2 = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
    "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings2, self.X), perm=[1, 0, 2])\n",
    "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings2, self.decoder_input), perm=[1, 0, 2])\n",
    "\n",
    "        with tf.name_scope(\"encoder\"):\n",
    "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n",
    "            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n",
    "\n",
    "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
    "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
    "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
    "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
    "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
    "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "            decoder_cell = self.cell(self.num_hidden * 2)\n",
    "\n",
    "            if not forward_only:\n",
    "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
    "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
    "                self.decoder_output = outputs.rnn_output\n",
    "                self.logits = tf.transpose(\n",
    "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
    "                self.logits_reshape = tf.concat(\n",
    "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
    "            else:\n",
    "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
    "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
    "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
    "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
    "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
    "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                    cell=decoder_cell,\n",
    "                    embedding=self.embeddings2,\n",
    "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
    "                    end_token=tf.constant(3),\n",
    "                    initial_state=initial_state,\n",
    "                    beam_width=self.beam_width,\n",
    "                    output_layer=self.projection_layer\n",
    "                )\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
    "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if not forward_only:\n",
    "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
    "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
    "                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n",
    "\n",
    "                params = tf.trainable_variables()\n",
    "                gradients = tf.gradients(self.loss, params)\n",
    "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from konlpy.tag import Twitter#t=Twitter()#tokens_ko=t.morphs(doc_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "def build_dict(step, toy=False):\n",
    "    if step == \"train\":\n",
    "      with open ('/home/lab11/0_project/dataset/news_sum2.csv', \"r\", encoding=\"utf-8\") as f:\n",
    "        head_li = []\n",
    "        topic_li = []\n",
    "        for x in f.readlines():\n",
    "          x = x.split('|')\n",
    "          if x[0]=='':\n",
    "            continue\n",
    "          if len(x)>2:\n",
    "            head_li.append(x[1])\n",
    "            topic_li.append(x[2])\n",
    "\n",
    "      words = list()\n",
    "      count = 0\n",
    "      dic = defaultdict(lambda:[])\n",
    "\n",
    "      for sentence in topic_li + head_li:\n",
    "          sentence = normalize(sentence, punctuation=True)\n",
    "          for idx,word in enumerate(sentence.split()):\n",
    "              if len(word) > 0:\n",
    "                  normalizedword=word[:3]\n",
    "                  tmp=[]\n",
    "                  for char in normalizedword:\n",
    "                      if ord(char) < 12593 or ord(char) > 12643:\n",
    "                          tmp.append(char)\n",
    "                  normalizedword = ''.join(char for char in tmp)\n",
    "                  if word not in dic:\n",
    "                      dic[normalizedword].append(word)\n",
    "\n",
    "      dic = sorted(dic.items(), key=operator.itemgetter(0))[1:]\n",
    "      words=[]\n",
    "      for i in range(len(dic)):\n",
    "          word=[]\n",
    "          word.append(dic[i][0])\n",
    "          for w in dic[i][1]:\n",
    "              if w not in word:\n",
    "                  word.append(w)\n",
    "          words.append(word)\n",
    "      words.append(['<padding>'])\n",
    "      words.append(['<unk>'])\n",
    "      words.append(['<s>'])\n",
    "      words.append(['</s>'])\n",
    "      \n",
    "      reversed_dict = {i:ch[0] for i,ch in enumerate(words)}\n",
    "      word_dict={}\n",
    "      for idx,words in enumerate(words):\n",
    "          for word in words:\n",
    "              word_dict[word]=idx\n",
    "              \n",
    "      with open(\"word_dict.pickle\", \"wb\") as f:\n",
    "          pickle.dump(word_dict, f)\n",
    "      with open(\"ix_to_dict.pickle\", \"wb\") as t:\n",
    "          pickle.dump(word_dict, t)\n",
    "\n",
    "    elif step == \"valid\":\n",
    "      with open(\"word_dict.pickle\", \"rb\") as f:\n",
    "          word_dict = pickle.load(f)\n",
    "\n",
    "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    article_max_len = 120\n",
    "    summary_max_len = 18\n",
    "    print(\"reversed dict:\",len(reversed_dict),\"word dict:\",len(word_dict))\n",
    "    return word_dict, reversed_dict, article_max_len, summary_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(step, word_dict, article_max_len, summary_max_len, toy=False):\n",
    "    if step == \"train\":\n",
    "      with open ('/home/lab11/0_project/dataset/news_sum2.csv', \"r\", encoding=\"utf-8\") as f:\n",
    "        head_li = []\n",
    "        topic_li = []\n",
    "        for x in f.readlines():\n",
    "          x = x.split('|')\n",
    "          if x[0]=='':\n",
    "            continue\n",
    "          if len(x)>2:\n",
    "            head_li.append(x[1])\n",
    "            topic_li.append(x[2])\n",
    "    elif step == \"valid\":\n",
    "      with open ('/home/lab11/0_project/dataset/news_sum2.csv', \"r\", encoding=\"utf-8\") as f:\n",
    "        topic_li = []\n",
    "        for x in f.readlines():\n",
    "          x = x.split('|')\n",
    "          if x[0]=='':\n",
    "            continue\n",
    "          if len(x)>2:\n",
    "            topic_li.append(x[2])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x = [normalize(d) for d in topic_li]\n",
    "    x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
    "    x = [d[:article_max_len] for d in x]\n",
    "    x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
    "    \n",
    "    if step == \"valid\":\n",
    "        return x\n",
    "    else:        \n",
    "        y = [normalize(d) for d in head_li]\n",
    "        y = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in y]\n",
    "        y = [d[:(summary_max_len - 1)] for d in y]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_embedding(reversed_dict, embedding_size):\n",
    "    glove_file = \"/home/lab11/0_project/glove/glove.42B.300d.txt\"\n",
    "    word2vec_file = get_tmpfile(\"word2vec_format.vec\")\n",
    "    glove2word2vec(glove_file, word2vec_file)\n",
    "    print(\"Loading Glove vectors...\")\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n",
    "\n",
    "    word_vec_list = list()\n",
    "    for _, word in sorted(reversed_dict.items()):\n",
    "        try:\n",
    "            word_vec = word_vectors.word_vec(word)\n",
    "        except KeyError:\n",
    "            word_vec = np.zeros([embedding_size], dtype=np.float32)\n",
    "\n",
    "        word_vec_list.append(word_vec)\n",
    "\n",
    "    # Assign random vector to <s>, </s> token\n",
    "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
    "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
    "\n",
    "    return np.array(word_vec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_iter(x, y, batch_size=32, num_epoch=10):\n",
    "#         x = np.array(x)\n",
    "#         y = np.array(y)\n",
    "#         for _ in range(num_epoch):\n",
    "#             for batch_num in range(0, len(x), batch_size):\n",
    "#                 yield x[batch_num:(batch_num+batch_size)], y[batch_num:(batch_num+batch_size)]\n",
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "    print(inputs)\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for _ in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            # start_index = batch_num * batch_size\n",
    "            # end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            # yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
    "            yield inputs[batch_num:(batch_num+batch_size)], outputs[batch_num:(batch_num+batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building dictionary...\n",
      "reversed dict: 67802 word dict: 124368\n",
      "Loading training dataset...\n"
     ]
    }
   ],
   "source": [
    "embedding_size=300\n",
    "num_hidden = 300\n",
    "num_layers = 3\n",
    "learning_rate = 0.001\n",
    "beam_width = 10\n",
    "keep_prob = 0.8\n",
    "glove = True\n",
    "batch_size=256\n",
    "num_epochs=10\n",
    "\n",
    "if not os.path.exists(\"saved_model\"):\n",
    "    os.mkdir(\"saved_model\")\n",
    "else:\n",
    "    old_model_checkpoint_path = open('saved_model/checkpoint', 'r')\n",
    "    old_model_checkpoint_path = \"\".join([\"saved_model/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1]])\n",
    "\n",
    "print(\"Building dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", toy=True)\n",
    "print(\"Loading training dataset...\")\n",
    "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, toy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Glove vectors...\n",
      "WARNING:tensorflow:From <ipython-input-7-aa546f50c877>:39: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/rnn/python/ops/rnn.py:239: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6b1eef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6b1eef90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6b1eef90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6b1eef90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfda90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfda90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfda90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfda90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fd9757bc090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fd9757bc090>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fd9757bc090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fd9757bc090>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfdd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfdd50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfdd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfdd50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfdf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfdf10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfdf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfdf10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfd790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfd790>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfd790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fda6acfd790>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd96e3da050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd96e3da050>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd96e3da050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd96e3da050>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fda69b35a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fda69b35a90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fda69b35a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7fda69b35a90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fdab04f3c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fdab04f3c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fdab04f3c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fdab04f3c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fda6b236590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fda6b236590>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fda6b236590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fda6b236590>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fda69f5c9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fda69f5c9d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fda69f5c9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fda69f5c9d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fda6b1eedd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fda6b1eedd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fda6b1eedd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fda6b1eedd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-7-aa546f50c877>:100: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "\n",
      "Iteration starts.\n",
      "Number of batches per epoch : 18\n",
      " Epoch 1: Model is saved. Elapsed: 00:19:52.39 \n",
      "\n",
      " Epoch 2: Model is saved. Elapsed: 00:28:27.21 \n",
      "\n",
      " Epoch 3: Model is saved. Elapsed: 00:37:02.69 \n",
      "\n",
      " Epoch 4: Model is saved. Elapsed: 00:45:40.67 \n",
      "\n",
      " Epoch 5: Model is saved. Elapsed: 00:54:16.70 \n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      " Epoch 6: Model is saved. Elapsed: 01:02:54.66 \n",
      "\n",
      " Epoch 7: Model is saved. Elapsed: 01:11:36.12 \n",
      "\n",
      " Epoch 8: Model is saved. Elapsed: 01:20:11.36 \n",
      "\n",
      " Epoch 9: Model is saved. Elapsed: 01:28:48.31 \n",
      "\n",
      " Epoch 10: Model is saved. Elapsed: 01:37:23.86 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, embedding_size, num_hidden, num_layers, learning_rate, beam_width, keep_prob, glove)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    if 'old_model_checkpoint_path' in globals():\n",
    "        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
    "        saver.restore(sess, old_model_checkpoint_path )\n",
    "\n",
    "    batches = batch_iter(train_x, train_y, batch_size, num_epochs)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // batch_size + 1\n",
    "\n",
    "    print(\"\\nIteration starts.\")\n",
    "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
    "    for batch_x, batch_y in batches:\n",
    "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
    "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
    "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
    "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
    "\n",
    "        batch_decoder_input = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
    "        batch_decoder_output = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
    "\n",
    "        train_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "            model.decoder_input: batch_decoder_input,\n",
    "            model.decoder_len: batch_decoder_len,\n",
    "            model.decoder_target: batch_decoder_output\n",
    "        }\n",
    "\n",
    "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(\"step {0}: loss = {1}\".format(step, loss))\n",
    "\n",
    "        if step % num_batches_per_epoch == 0:\n",
    "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
    "            minutes, seconds = divmod(rem, 60)\n",
    "            saver.save(sess, \"./saved_model/model.ckpt\", global_step=step)\n",
    "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
    "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "def headline(data):\n",
    "    \n",
    "    embedding_size=300\n",
    "    num_hidden = 300\n",
    "    num_layers = 3\n",
    "    learning_rate = 0.001\n",
    "    beam_width = 10\n",
    "    keep_prob = 0.8\n",
    "    glove = True\n",
    "    batch_size=128\n",
    "    num_epochs=15\n",
    "    while True:\n",
    "        print(\"Loading dictionary...\")\n",
    "        word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", False)\n",
    "        print('word dict : ',word_dict)\n",
    "        content=normalize(data)\n",
    "        print('content : ', content)\n",
    "        x = [normalize(content)]\n",
    "        x = [word_dict.get(w, word_dict[\"<unk>\"]) for w in x]\n",
    "        # x = x[:120]\n",
    "        print('x : ',x)\n",
    "        # valid_x = x + (article_max_len - len(x)) * [word_dict[\"<padding>\"]]\n",
    "        valid_x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
    "        # valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
    "        print('valid_x : ',valid_x)\n",
    "        with tf.Session() as sess:\n",
    "          print(\"Loading saved model...\")\n",
    "          model = Model(reversed_dict, article_max_len, summary_max_len, embedding_size, num_hidden, num_layers, learning_rate, beam_width, keep_prob, glove, forward_only=True)\n",
    "          saver = tf.train.Saver(tf.global_variables())\n",
    "          ckpt = tf.train.get_checkpoint_state(\"./saved_model/\")\n",
    "          saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "          batches = batch_iter(valid_x, [0] * len(valid_x), batch_size, 1)\n",
    "\n",
    "          for batch_x, _ in batches:\n",
    "              print(batch_x)\n",
    "              batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
    "            #   batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "\n",
    "              valid_feed_dict = {\n",
    "                  model.batch_size: len(batch_x),\n",
    "                  model.X: batch_x,\n",
    "                  model.X_len: batch_x_len,\n",
    "              }\n",
    "\n",
    "              prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "              prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "              summary=list()\n",
    "              predict=\"\"\n",
    "              for word in prediction_output:\n",
    "                  if word == \"</s>\":\n",
    "                      break\n",
    "                  if word not in summary:\n",
    "                      summary.append(word)\n",
    "                  predict=\" \".join(summary)\n",
    "              print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexrankr import LexRank\n",
    "\n",
    "def summary(data):\n",
    "    print(\"smry start\")\n",
    "    multi_summary = data\n",
    "    \n",
    "    lexrank = LexRank()\n",
    "    lexrank.summarize(multi_summary)  # data (본문)가져와서 요약\n",
    "    summaries = lexrank.probe(3)  # 3줄요약, summaries 타입은 list\n",
    "    summaries = '. '.join(summaries)+'.'\n",
    "    print(\"multi-summary= \",summaries)\n",
    "    return summaries\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "고했': 67014, '회고했다.': 67014, '회관에': 67015, '회관에서는': 67015, '회귀는': 67016, '회귀에': 67017, '회귀한': 67018, '회기에': 67019, '회기에서': 67019, '회담': 67020, '회담과': 67021, '회담에': 67022, '회담에서': 67022, '회담에서도': 67022, '회담에선': 67022, '회담은': 67023, '회담을': 67024, '회담의': 67025, '회담이': 67026, '회답해': 67027, '회답해주시기': 67027, '회동': 67028, '회동도': 67029, '회동에': 67030, '회동에서': 67030, '회동에도': 67030, '회동은': 67031, '회동을': 67032, '회동하': 67033, '회동하면서': 67033, '회동해': 67034, '회동해,': 67034, '회람': 67035, '회랑': 67036, '회령시': 67037, '회령시에': 67037, '회룡천': 67038, '회룡천에서': 67038, '회룡천과': 67038, '회복': 67039, '회복되': 67040, '회복되면서': 67040, '회복되지는': 67040, '회복되고': 67040, '회복되지': 67040, '회복되긴': 67040, '회복때': 67041, '회복세': 67042, '회복세로': 67042, '회복세를': 67042, '회복세임에도': 67042, '회복세가': 67042, '회복시': 67043, '회복시키는': 67043, '회복에': 67044, '회복은': 67045, '회복을': 67046, '회복의': 67047, '회복이': 67048, '회복하': 67049, '회복하면서': 67049, '회복하는': 67049, '회복하면': 67049, '회복하고': 67049, '회복하기': 67049, '회복하고,': 67049, '회복하겠다': 67049, '회복하지': 67049, '회복할': 67050, '회복해': 67051, '회복했': 67052, '회복했다.': 67052, '회복했으며,': 67052, '회비': 67053, '회비관': 67054, '회비관리': 67054, '회비로': 67055, '회사': 67056, '회사가': 67057, '회사는': 67058, '회사도': 67059, '회사들': 67060, '회사들은': 67060, '회사들이': 67060, '회사들에서': 67060, '회사들에': 67060, '회사로': 67061, '회사로부터': 67061, '회사를': 67062, '회사명': 67063, '회사명과': 67063, '회사에': 67064, '회사에서': 67064, '회사엔': 67065, '회사와': 67066, '회사원': 67067, '회사원을': 67067, '회사의': 67068, '회사인': 67069, '회사채': 67070, '회사채에': 67070, '회사채와': 67070, '회사처': 67071, '회사처럼': 67071, '회삿돈': 67072, '회삿돈을': 67072, '회상하': 67073, '회상하는': 67073, '회색': 67074, '회색,': 67075, '회색인': 67076, '회색인들의': 67076, '회생계': 67077, '회생계획안을': 67077, '회생안': 67078, '회생안을': 67078, '회생여': 67079, '회생여부는': 67079, '회생을': 67080, '회생의': 67081, '회선': 67082, '회선의': 67083, '회수': 67084, '회수가': 67085, '회수되': 67086, '회수되고': 67086, '회수하': 67087, '회수하고': 67087, '회수하기': 67087, '회수하다가': 67087, '회수할': 67088, '회식': 67089, '회식비': 67090, '회식자': 67091, '회식자리에': 67091, '회신을': 67092, '회심의': 67093, '회씩': 67094, '회오리': 67095, '회오리를': 67095, '회원': 67096, '회원,': 67097, '회원가': 67098, '회원가입이나': 67098, '회원과': 67099, '회원국': 67100, '회원국의': 67100, '회원국으로': 67100, '회원국이': 67100, '회원국들에게': 67100, '회원사': 67101, '회원사인': 67101, '회원으': 67102, '회원으로,': 67102, '회원으로': 67102, '회원이': 67103, '회원정': 67104, '회원정보': 67104, '회유': 67105, '회유하': 67106, '회유하는': 67106, '회의': 67107, \"회의'\": 67108, \"회의'에서\": 67108, '회의,': 67109, '회의.': 67110, '회의가': 67111, '회의규': 67112, '회의규칙': 67112, '회의도': 67113, '회의로': 67114, '회의록': 67115, '회의록,': 67115, '회의록의': 67115, \"회의록'의\": 67115, '회의록을': 67115, '회의론': 67116, '회의론자': 67116, '회의론도': 67116, '회의를': 67117, '회의비': 67118, '회의에': 67119, '회의에서': 67119, '회의에서도': 67119, '회의에서는': 67119, '회의였': 67120, '회의였다.': 67120, '회의와': 67121, '회의의': 67122, '회의장': 67123, '회의장에서': 67123, '회의장이': 67123, '회의적': 67124, '회의적인': 67124, '회의체': 67125, '회의체인': 67125, '회장': 67126, '회장,': 67127, '회장.': 67128, '회장과': 67129, '회장과의': 67129, '회장국': 67130, '회장국으로': 67130, '회장님': 67131, '회장님이': 67131, '회장도': 67132, '회장만': 67133, '회장만이': 67133, '회장배': 67134, '회장에': 67135, '회장에게': 67135, '회장에게서': 67135, '회장으': 67136, '회장으로': 67136, '회장으로부터': 67136, '회장은': 67137, '회장을': 67138, '회장의': 67139, '회장이': 67140, '회장이다.': 67140, '회장인': 67141, '회장직': 67142, '회장직은': 67142, '회장처': 67143, '회장처럼': 67143, '회장한': 67144, '회장한테서': 67144, '회전문': 67145, '회전수': 67146, '회전수가': 67146, '회전율': 67147, '회전율은': 67147, '회전이': 67148, '회전하': 67149, '회전하는': 67149, '회전해': 67150, '회초리': 67151, '회초리로': 67151, '회춘한': 67152, '회피': 67153, '회피로': 67154, '회피를': 67155, '회피하': 67156, '회피하게': 67156, '회피하고': 67156, '회피하려고': 67156, '회피했': 67157, '회피했다.': 67157, '회한': 67158, '회항': 67159, '회항시': 67160, '회항시키고': 67160, '회항했': 67161, '회항했다가': 67161, '회화': 67162, '회화가': 67163, '회화를': 67164, '획': 67165, '획기적': 67166, '획기적으로': 67166, '획기적인': 67166, '획득을': 67167, '획득하': 67168, '획득하는': 67168, '획득하고': 67168, '획득한': 67169, '획득했': 67170, '획득했다.': 67170, '획일적': 67171, '획일적인': 67171, '획일적으로': 67171, '획일화': 67172, '획일화의': 67172, '획정': 67173, '획정안': 67174, '획정안이': 67174, '획정위': 67175, '획정위는': 67175, '획정위에서는': 67175, '획정을': 67176, '횟수': 67177, '횟수나': 67178, '횟수에': 67179, '횟수에선': 67179, '횟집을': 67180, '횡계리': 67181, '횡계리에': 67181, '횡단': 67182, '횡단도': 67183, '횡단보': 67184, '횡단보도,': 67184, '횡단보도': 67184, '횡단보도에서는': 67184, '횡단한': 67185, '횡령': 67186, '횡령,': 67187, '횡령과': 67188, '횡령사': 67189, '횡령사실을': 67189, '횡령이': 67190, '횡령이나': 67190, '횡령자': 67191, '횡령자금을': 67191, '횡령자금': 67191, '횡령하': 67192, '횡령하는': 67192, '횡령한': 67193, '횡령했': 67194, '횡령했다는': 67194, '횡령했다가': 67194, '횡령혐': 67195, '횡령혐의': 67195, '횡무진': 67196, '횡보장': 67197, '횡보장세와는': 67197, '횡성의': 67198, '횡포': 67199, '횡포\"': 67200, '횡포\"\"라며': 67200, \"횡포'\": 67201, '횡포를': 67202, '횡포에': 67203, '횡포에서': 67203, '횡행하': 67204, '횡행하고': 67204, '효과': 67205, '효과?': 67206, '효과가': 67207, '효과는': 67208, '효과도': 67209, '효과를': 67210, '효과만': 67211, '효과성': 67212, '효과성을': 67212, '효과에': 67213, '효과에도': 67213, '효과의': 67214, '효과적': 67215, '효과적이다.': 67215, '효과적이라는': 67215, '효과적으로': 67215, '효과적인': 67215, '효과적일': 67215, '효과적이지': 67215, '효도': 67216, '효도신': 67217, '효도신발': 67217, '효도하': 67218, '효도하려나': 67218, '효력을': 67219, '효력이': 67220, '효력정': 67221, '효력정지': 67221, '효성그': 67222, '효성그룹': 67222, '효성그룹의': 67222, '효성에': 67223, '효성여': 67224, '효성여대': 67224, '효성은': 67225, '효성을': 67226, '효성의': 67227, '효성측': 67228, '효성측의': 67228, '효용은': 67229, '효용을': 67230, '효율': 67231, '효율로': 67232, '효율성': 67233, '효율성이': 67233, '효율적': 67234, '효율적으로': 67234, '효율적인': 67234, '효율적이라는': 67234, '효율적이다.': 67234, '효은': 67235, '효자': 67236, '효천고': 67237, '효천역': 67238, '효천역세권을': 67238, '후': 67239, '후,': 67240, '후각': 67241, '후견인': 67242, '후계구': 67243, '후계구도': 67243, '후계목': 67244, '후계목을': 67244, '후계자': 67245, '후계자인': 67245, '후계자로': 67245, '후계자는': 67245, '후고': 67246, '후광으': 67247, '후광으로': 67247, '후광을': 67248, '후광효': 67249, '후광효과를': 67249, '후기': 67250, '후기를': 67251, '후기의': 67252, '후끈': 67253, '후난': 67254, '후난성': 67255, '후두염': 67256, '후두염,': 67256, '후레쉬': 67257, '후레쉬센터는': 67257, '후레쉬센터에': 67257, '후로도': 67258, '후마니': 67259, '후마니타스': 67259, '후문으': 67260, '후문으로': 67260, '후반': 67261, '후반,': 67262, '후반기': 67263, '후반기를': 67263, '후반기는': 67263, '후반기에': 67263, '후반에': 67264, '후반에서': 67264, '후반으': 67265, '후반으로': 67265, '후반의': 67266, '후반전': 67267, '후반전,': 67267, '후발업': 67268, '후발업체들은': 67268, '후발업체의': 67268, '후발이': 67269, '후발이건': 67269, '후발주': 67270, '후발주자로': 67270, '후발주자였던': 67270, '후방': 67271, '후배': 67272, '후배들': 67273, '후배들에게': 67273, '후배들을': 67273, '후배들의': 67273, '후배들은': 67273, '후배를': 67274, '후배인': 67275, '후베닐': 67276, '후베닐A의': 67276, '후베이': 67277, '후베이성의': 67277, '후보': 67278, '후보가': 67279, '후보군': 67280, '후보군에': 67280, '후보그': 67281, '후보그룹으로': 67281, '후보까': 67282, '후보까지': 67282, '후보는': 67283, '후보단': 67284, '후보단일화': 67284, '후보도': 67285, '후보들': 67286, '후보들에': 67286, '후보들이': 67286, '후보들을': 67286, '후보들의': 67286, '후보들도': 67286, '후보들은': 67286, '후보들이다.': 67286, '후보들에게': 67286, '후보들과': 67286, '후보들과의': 67286, '후보라': 67287, '후보라도': 67287, '후보로': 67288, '후보로는': 67288, '후보를': 67289, '후보마': 67290, '후보마다': 67290, '후보만': 67291, '후보보': 67292, '후보보다': 67292, '후보사': 67293, '후보사퇴': 67293, '후보에': 67294, '후보에게': 67294, '후보였': 67295, '후보였던': 67295, '후보와': 67296, '후보와의': 67296, '후보의': 67297, '후보인': 67298, '후보인지': 67298, '후보자': 67299, '후보자가': 67299, '후보자는': 67299, '후보자로서는': 67299, '후보자도': 67299, '후보자의': 67299, '후보자와': 67299, '후보자로': 67299, '후보자에': 67299, '후보자들을': 67299, '후보자를': 67299, \"후보자'의\": 67299, '후보자들은': 67299, '후보자에게': 67299, '후보자인': 67299, '후보자엔': 67299, '후보자,': 67299, '후보작': 67300, '후보작에': 67300, '후보지': 67301, '후보지는': 67301, '후보지가': 67301, '후보직': 67302, '후보직에서': 67302, '후보직을': 67302, '후보쪽': 67303, '후보쪽,': 67303, '후보추': 67304, '후보추천위원회는': 67304, '후보확': 67305, '후보확정': 67305, '후부터': 67306, '후비거': 67307, '후비거나': 67307, '후생노': 67308, '후생노동상이': 67308, '후속': 67309, '후속대': 67310, '후속대책': 67310, '후속작': 67311, '후속작업을': 67311, '후속편': 67312, '후속편으로': 67312, '후속편의': 67312, '후손': 67313, '후손과': 67314, '후손이': 67315, '후손인': 67316, '후송까': 67317, '후송까지': 67317, '후송을': 67318, '후순위': 67319, '후순위채는': 67319, '후순위채가': 67319, '후순위채의': 67319, '후안': 67320, '후암동': 67321, '후야오': 67322, '후야오방': 67322, '후에': 67323, '후에는': 67324, '후에도': 67325, '후엔': 67326, '후예라': 67327, '후원': 67328, '후원금': 67329, '후원금으로': 67329, '후원금의': 67329, '후원금을': 67329, '후원금,': 67329, '후원모': 67330, '후원모집': 67330, '후원병': 67331, '후원병원으로': 67331, '후원사': 67332, '후원사들의': 67332, '후원에': 67333, '후원을': 67334, '후원의': 67335, '후원이': 67336, '후원자': 67337, '후원자들은': 67337, '후원자들에': 67337, '후원자가': 67337, '후원하': 67338, '후원하고': 67338, '후원하기로': 67338, '후원해': 67339, '후원회': 67340, '후유장': 67341, '후유장애': 67341, '후유증': 67342, '후유증에': 67342, '후유증이': 67342, '후유증을': 67342, '후의': 67343, '후이저': 67344, '후이저우시의': 67344, '후인': 67345, '후임': 67346, '후임에': 67347, '후임은': 67348, '후임인': 67349, '후임자': 67350, '후임자를': 67350, '후줄근': 67351, '후줄근한': 67351, '후지쓰': 67352, '후지쓰배': 67352, '후지의': 67353, '후지티': 67354, '후지티브이': 67354, '후지필': 67355, '후지필름의': 67355, '후진': 67356, '후진적': 67357, '후진적인': 67357, '후진타': 67358, '후진타오': 67358, '후진타오의': 67358, '후진타오,': 67358, '후진하': 67359, '후진하는': 67359, '후쯤': 67360, '후춧가': 67361, '후춧가루,': 67361, '후치령': 67362, '후치령의': 67362, '후쿠다': 67363, '후쿠시': 67364, '후쿠시마': 67364, '후쿠오': 67365, '후쿠오카': 67365, '후쿠자': 67366, '후쿠자와와': 67366, '후텐마': 67367, '후퇴': 67368, '후퇴란': 67369, '후퇴를': 67370, '후퇴한': 67371, '후퇴해': 67372, '후티': 67373, '후폭풍': 67374, '후폭풍은': 67374, '후폭풍도': 67374, '후폭풍을': 67374, '후폭풍이': 67374, '후폭풍에': 67374, '후프': 67375, '후학을': 67376, '후한': 67377, '후회': 67378, '후회하': 67379, '후회하게': 67379, '훈': 67380, '훈계하': 67381, '훈계하기로': 67381, '훈남': 67382, '훈련': 67383, '훈련\"': 67384, '훈련,': 67385, '훈련기': 67386, '훈련도': 67387, '훈련받': 67388, '훈련받던': 67388, '훈련시': 67389, '훈련시켜': 67389, '훈련시켜왔다.': 67389, '훈련에': 67390, '훈련에서': 67390, '훈련은': 67391, '훈련을': 67392, '훈련의': 67393, '훈련이': 67394, '훈련장': 67395, '훈련장에': 67395, '훈련장비': 67395, '훈련장에서': 67395, '훈련하': 67396, '훈련하고': 67396, '훈련하겠다는': 67396, '훈련하는': 67396, '훈련한': 67397, '훈련할': 67398, '훈련했': 67399, '훈련했던': 67399, '훈련했다.': 67399, '훈민정': 67400, '훈민정음': 67400, '훈민정음의': 67400, '훈수': 67401, '훈수와': 67402, '훈시하': 67403, '훈시하는': 67403, '훈장을': 67404, '훈춘을': 67405, '훈풍': 67406, '훈풍의': 67407, '훈풍이': 67408, '훈훈한': 67409, '훌륭하': 67410, '훌륭하게': 67410, '훌륭하지만': 67410, '훌륭한': 67411, '훌쩍': 67412, '훑기': 67413, '훑어보': 67414, '훑어보라는': 67414, '훑어보라': 67414, '훑었다': 67415, '훑었다.': 67415, '훔쳐': 67416, '훔쳐가': 67417, '훔쳐가요': 67417, '훔쳐간': 67418, '훔쳐야': 67419, '훔치거': 67420, '훔치거나': 67420, '훔치는': 67421, '훔치다': 67422, '훔친': 67423, '훗날': 67424, '훙': 67425, '훨씬': 67426, '훨훨': 67427, '훼손': 67428, '훼손\"': 67429, '훼손,': 67430, '훼손과': 67431, '훼손당': 67432, '훼손당한': 67432, '훼손됐': 67433, '훼손됐을': 67433, '훼손되': 67434, '훼손되고': 67434, '훼손되지': 67434, '훼손된': 67435, '훼손될': 67436, '훼손을': 67437, '훼손이': 67438, '훼손하': 67439, '훼손하는': 67439, '훼손하고': 67439, '훼손하지': 67439, '훼손하겠다며': 67439, '훼손한': 67440, '훼손한다는': 67440, '휘고': 67441, '휘닉스': 67442, '휘닉스파크': 67442, '휘두르': 67443, '휘두르며': 67443, '휘두르고': 67443, '휘두른': 67444, '휘둘렀': 67445, '휘둘렀다': 67445, '휘둘렀고,': 67445, '휘말려': 67446, '휘말렸': 67447, '휘말렸던': 67447, '휘말렸다.': 67447, '휘말리': 67448, '휘말리는': 67448, '휘말린': 67449, '휘말릴': 67450, '휘몰아': 67451, '휘몰아치고': 67451, '휘발유': 67452, '휘발유,': 67452, '휘발유를': 67452, '휘발유값은': 67452, '휘발유값': 67452, '휘센': 67453, '휘슬링': 67454, '휘슬링스트레이츠': 67454, '휘어져': 67455, '휘어지': 67456, '휘어지고': 67456, '휘어진': 67457, '휘영청': 67458, '휘저어': 67459, '휘젓고': 67460, '휘청': 67461, '휘청거': 67462, '휘청거리고': 67462, '휘청대': 67463, '휘청대는': 67463, '휘트니': 67464, '휘트니미술관전': 67464, '휘트니미술관': 67464, '휜': 67465, '휠체어': 67466, '휠체어컬링': 67466, '휠체어컬링을': 67466, '휩싸여': 67467, '휩싸였': 67468, '휩싸였다.': 67468, '휩싸이': 67469, '휩싸이며': 67469, '휩싸이기': 67469, '휩싸인': 67470, '휩쓴': 67471, '휩쓴다': 67472, '휩쓸': 67473, '휩쓸던': 67474, '휩쓸리': 67475, '휩쓸리면서': 67475, '휩쓸며': 67476, '휩쓸었': 67477, '휩쓸었다.': 67477, '휩쓸줄': 67478, '휴': 67479, '휴가': 67480, '휴가,': 67481, '휴가가': 67482, '휴가간': 67483, '휴가떠': 67484, '휴가떠나': 67484, '휴가를': 67485, '휴가시': 67486, '휴가시즌이': 67486, '휴가에': 67487, '휴가에도': 67487, '휴가철': 67488, '휴가철을': 67488, '휴게': 67489, '휴게공': 67490, '휴게공간': 67490, '휴게공간을': 67490, '휴게소': 67491, '휴게소에서': 67491, '휴게소라고': 67491, '휴게시': 67492, '휴게시설,': 67492, '휴게실': 67493, '휴게실에서': 67493, '휴고': 67494, '휴관': 67495, '휴대성': 67496, '휴대성은': 67496, '휴대용': 67497, '휴대전': 67498, '휴대전화인': 67498, '휴대전화가': 67498, '휴대전화용': 67498, '휴대전화': 67498, '휴대전화번호를': 67498, '휴대전화로': 67498, '휴대전화에서': 67498, '휴대전화를': 67498, '휴대전화는': 67498, '휴대전화의': 67498, '휴대전화에': 67498, '휴대전화만': 67498, '휴대전화나': 67498, '휴대전화,': 67498, '휴대폰': 67499, '휴대폰으로': 67499, '휴대폰을': 67499, '휴대폰요금': 67499, '휴대하': 67500, '휴대하고': 67500, '휴먼': 67501, '휴무일': 67502, '휴스턴': 67503, '휴스턴,': 67503, '휴스턴에서': 67503, '휴식': 67504, '휴식기': 67505, '휴식기를': 67505, '휴식기에도': 67505, '휴식을': 67506, '휴심정': 67507, '휴심정에': 67507, '휴양': 67508, '휴양지': 67509, '휴양지에서': 67509, '휴양지인': 67509, '휴양지서': 67509, '휴양지로': 67509, '휴업': 67510, '휴업,': 67511, '휴업을': 67512, '휴업의': 67513, '휴업하': 67514, '휴업하면서': 67514, '휴일': 67515, '휴일에': 67516, '휴일에도': 67516, '휴일이': 67517, '휴일이거나': 67517, '휴일인': 67518, '휴장하': 67519, '휴장하고': 67519, '휴전': 67520, '휴전선': 67521, '휴전선에서': 67521, '휴전선내': 67521, '휴전성': 67522, '휴전성명': 67522, '휴전에': 67523, '휴전을': 67524, '휴지': 67525, '휴직': 67526, '휴직은': 67527, '휴직을': 67528, '휴직했': 67529, '휴직했다.': 67529, '휴진': 67530, '휴진한': 67531, '휴학': 67532, '휴학하': 67533, '휴학하기로': 67533, '휴회': 67534, '흉강경': 67535, '흉강경을': 67535, '흉금을': 67536, '흉기': 67537, '흉기로': 67538, '흉기를': 67539, '흉기에': 67540, '흉부외': 67541, '흉부외과': 67541, '흉부외과를': 67541, '흉상': 67542, '흉터': 67543, '흉터를': 67544, '흐느낌': 67545, '흐느낌이': 67545, '흐르고': 67546, '흐르는': 67547, '흐르듯': 67548, '흐르듯해': 67548, '흐른': 67549, '흐른다': 67550, '흐른다고': 67550, '흐름과': 67551, '흐름에': 67552, '흐름을': 67553, '흐름을,': 67553, '흐름이': 67554, '흐름이나': 67554, '흐엉,': 67555, '흐엉의': 67556, '흐지부': 67557, '흐지부지되는': 67557, '흐트러': 67558, '흐트러져': 67558, '흑금성': 67559, '흑돌': 67560, '흑백': 67561, '흑백영': 67562, '흑백영화': 67562, '흑색선': 67563, '흑색선전을': 67563, '흑인': 67564, '흑인과': 67565, '흑인교': 67566, '흑인교회에서': 67566, '흑인교회': 67566, '흑인들': 67567, '흑인뿐': 67568, '흑인뿐만': 67568, '흑인은': 67569, '흑인의': 67570, '흑인장': 67571, '흑인장관에': 67571, '흑자': 67572, '흑자가': 67573, '흑자로': 67574, '흑자를': 67575, '흑자전': 67576, '흑자전환': 67576, '흑자폭': 67577, '흑자폭을': 67577, '흑자폭이': 67577, '흔드는': 67578, '흔든': 67579, '흔든다': 67580, '흔들': 67581, '흔들고': 67582, '흔들기': 67583, '흔들기는': 67583, '흔들려': 67584, '흔들리': 67585, '흔들리고': 67585, '흔들리지': 67585, '흔들리던': 67585, '흔들리는': 67585, '흔들리게': 67585, '흔들린': 67586, '흔들릴': 67587, '흔들림': 67588, '흔들림은': 67588, '흔들며': 67589, '흔들면': 67590, '흔들어': 67591, '흔들었': 67592, '흔들었다': 67592, '흔적없': 67593, '흔적없다': 67593, '흔적은': 67594, '흔적을': 67595, '흔적이': 67596, '흔쾌히': 67597, '흔한': 67598, '흔한데': 67599, '흔한데,': 67599, '흔해빠': 67600, '흔해빠진': 67600, '흔히': 67601, '흘러간': 67602, '흘러간다는': 67602, '흘러갈': 67603, '흘러갔': 67604, '흘러갔는지': 67604, '흘러나': 67605, '흘러나온': 67605, '흘러나왔고,': 67605, '흘러나오자,': 67605, '흘러나온다': 67605, '흘러드': 67606, '흘러드는': 67606, '흘러들': 67607, '흘러들어간': 67607, '흘렀다': 67608, '흘렀다.': 67608, '흘려': 67609, '흘려도': 67610, '흘려보': 67611, '흘려보내는': 67611, '흘렸는': 67612, '흘렸는데': 67612, '흘렸다': 67613, '흘렸다.': 67613, '흘리고': 67614, '흘리고,': 67614, '흘리는': 67615, '흘리도': 67616, '흘리도록': 67616, '흘리며': 67617, '흘린': 67618, '흘린다': 67619, '흘린다고': 67619, '흘릴': 67620, '흙': 67621, '흙을': 67622, '흙집': 67623, '흙집을': 67624, '흙층이': 67625, '흙탕물': 67626, '흙탕물에': 67626, '흠뻑\"': 67627, '흠집과': 67628, '흠집을': 67629, '흠집이': 67630, '흡사': 67631, '흡수': 67632, '흡수가': 67633, '흡수를': 67634, '흡수율': 67635, '흡수율이': 67635, '흡수하': 67636, '흡수하는,': 67636, '흡수할': 67637, '흡수해': 67638, '흡연': 67639, '흡연율': 67640, '흡연율을': 67640, '흡연으': 67641, '흡연으로': 67641, '흡연은': 67642, '흡연의': 67643, '흡연이': 67644, '흡연자': 67645, '흡연자가': 67645, '흡연자들은': 67645, '흡연층': 67646, '흡입독': 67647, '흡입독성': 67647, '흡입할': 67648, '흡착제': 67649, '흡착제를': 67649, '흥겨운': 67650, '흥겹고': 67651, '흥국생': 67652, '흥국생명을': 67652, '흥국생명': 67652, '흥국생명에': 67652, '흥남철': 67653, '흥남철수': 67653, '흥망': 67654, '흥미': 67655, '흥미로': 67656, '흥미로웠다.': 67656, '흥미로운': 67656, '흥미로워졌다.': 67656, '흥미롭': 67657, '흥미롭지만': 67657, '흥미롭다.': 67657, '흥미롭지만,': 67657, '흥미를': 67658, '흥미진': 67659, '흥미진진하게': 67659, '흥미진진해졌네': 67659, '흥분된': 67660, '흥분된다.': 67660, '흥분의': 67661, '흥분제': 67662, '흥분하': 67663, '흥분하면': 67663, '흥분했': 67664, '흥분했던지': 67664, '흥에': 67665, '흥월리': 67666, '흥월리층': 67666, '흥을': 67667, '흥진고': 67668, '흥천': 67669, '흥청망': 67670, '흥청망청': 67670, '흥행': 67671, '흥행대': 67672, '흥행대박,': 67672, '흥행에': 67673, '흥행영': 67674, '흥행영화': 67674, '흥행을': 67675, '흥행의': 67676, '흥행작': 67677, '흥행작을': 67677, '흥행한': 67678, '흥행했': 67679, '흥행했던': 67679, '희곡': 67680, '희곡상': 67681, '희곡상,': 67681, '희곡으': 67682, '희곡으로': 67682, '희귀': 67683, '희귀한': 67684, '희극에': 67685, '희극에서': 67685, '희극이': 67686, '희극이라고': 67686, '희극인': 67687, '희극인가.': 67687, '희랍어': 67688, '희로애': 67689, '희로애락을': 67689, '희망': 67690, '희망고': 67691, '희망고문': 67691, '희망교': 67692, '희망교육': 67692, '희망근': 67693, '희망근로와': 67693, '희망버': 67694, \"희망버스'\": 67694, '희망버스는': 67694, '희망속': 67695, '희망속에': 67695, '희망연': 67696, '희망연대노조': 67696, '희망은': 67697, '희망을': 67698, '희망의': 67699, '희망이': 67700, '희망이다': 67700, '희망임': 67701, '희망임대주택리츠': 67701, '희망자': 67702, '희망자들이': 67702, '희망퇴': 67703, '희망퇴직': 67703, '희망퇴직시킨': 67703, '희망플': 67704, '희망플랜은': 67704, '희망플랜을': 67704, '희망플랜': 67704, '희망하': 67705, '희망하는': 67705, '희망한': 67706, '희망한다': 67706, '희망한다면': 67706, '희망한다고': 67706, '희망한다\"\"고': 67706, '희미하': 67707, '희미하게': 67707, '희박한': 67708, '희박해': 67709, '희박해졌다.': 67709, '희비가': 67710, '희생': 67711, \"희생'\": 67712, \"희생'을\": 67712, '희생과': 67713, '희생당': 67714, '희생당한': 67714, '희생도': 67715, '희생됐': 67716, '희생됐다': 67716, '희생된': 67717, '희생시': 67718, '희생시키는': 67718, '희생시키지': 67718, '희생은': 67719, '희생을': 67720, '희생자': 67721, '희생자가': 67721, '희생자들의': 67721, '희생자는': 67721, '희생자의': 67721, '희생자들은': 67721, '희생자들도': 67721, '희생자를': 67721, '희생자와': 67721, '희생정': 67722, '희생정신': 67722, '희생정신에': 67722, '희생하': 67723, '희생하지': 67723, '희생하더라도': 67723, '희석시': 67724, '희석시키려는': 67724, '희석식': 67725, '희소성': 67726, '희소성으로': 67726, '희열을': 67727, '희유금': 67728, '희유금속': 67728, '희토류': 67729, '희토류가': 67729, '희토류만': 67729, '희토류를': 67729, '희토류의': 67729, '희한한': 67730, '희화화': 67731, '희화화하는': 67731, '흰돌고': 67732, '흰돌고래': 67732, '흰색': 67733, '히데오': 67734, '히데카': 67735, '히데카쓰': 67735, '히데키': 67736, '히로부': 67737, '히로부미가': 67737, '히로시': 67738, '히로시마': 67738, '히로시마공항에': 67738, '히로카': 67739, '히로카즈': 67739, '히말라': 67740, '히말라야는': 67740, '히말라야에서도': 67740, '히말라야의': 67740, '히말라야': 67740, '히메네': 67741, '히메네스': 67741, '히브리': 67742, '히브리어로': 67742, '히사시': 67743, '히스패': 67744, '히스패닉': 67744, '히스패닉계': 67744, '히스패닉계의': 67744, '히어로': 67745, '히어로즈': 67745, '히어로들이': 67745, '히터': 67746, '히트': 67747, '히트를': 67748, '히트시': 67749, '히트시킨': 67749, '히트시키며': 67749, '히트작': 67750, '히트작일': 67750, '히틀러': 67751, '힌두교': 67752, '힌두교와': 67752, '힌지': 67753, '힌트를': 67754, '힐': 67755, '힐난까': 67756, '힐난까지': 67756, '힐드로': 67757, '힐드로사이': 67757, '힐러리': 67758, '힐러리가': 67758, '힐러리는': 67758, '힐러리,': 67758, '힐리어': 67759, '힐리어드': 67759, '힐링이': 67760, '힐이': 67761, '힘': 67762, '힘,': 67763, '힘?': 67764, '힘겨루': 67765, '힘겨루기가': 67765, '힘겨루기를': 67765, '힘겨루기로': 67765, '힘겨루기': 67765, '힘겨운': 67766, '힘내라': 67767, '힘내세': 67768, '힘내세요': 67768, '힘도': 67769, '힘든': 67770, '힘들': 67771, '힘들게': 67772, '힘들겠': 67773, '힘들겠지만': 67773, '힘들고': 67774, '힘들기': 67775, '힘들기도': 67775, '힘들다': 67776, '힘들다면': 67776, '힘들다는': 67776, '힘들다.': 67776, '힘들다면서': 67776, '힘들다\"\"고': 67776, \"힘들다'는\": 67776, '힘들어': 67777, '힘들어질지': 67777, '힘들어도': 67777, '힘들어질': 67777, '힘들어진': 67777, '힘들어지는': 67777, '힘들어졌다고': 67777, '힘들었': 67778, '힘들었던': 67778, '힘들었다': 67778, '힘들었죠\"\"\"': 67778, '힘받는': 67779, '힘빠지': 67780, '힘빠지는': 67780, '힘빠진': 67781, '힘쓰겠': 67782, '힘쓰겠다.': 67782, '힘쓰고': 67783, '힘쓸': 67784, '힘없는': 67785, '힘으로': 67786, '힘은': 67787, '힘을': 67788, '힘의': 67789, '힘이': 67790, '힘이나': 67791, '힘이나마': 67791, '힘입어': 67792, '힘있는': 67793, '힘줄,': 67794, '힘찬': 67795, '힘찬병': 67796, '힘찬병원의': 67796, '힘찬병원': 67796, '힙합': 67797, '<padding>': 67798, '<unk>': 67799, '<s>': 67800, '</s>': 67801}\ncontent :  미국 제약회사 길리어드사이언스의 항바이러스제 렘데시비르 가 신종 코로나바이러스 감염증 코로나19 환자에게 미치는 효과가 거의 없다는 세계보건기구 WHO 의 연구 결과가 나왔다 WHO가 입원 환자 1만1천266명을 상대로 진행하고 있는 연대 실험 에서 렘데시비르가 환자의 입원 기간을 줄이거나 사망률을 낮추지 못했다고 로이터통신이 15일 보도했다 길리어드사이언스는 이달 초 코로나19 입원 환자 1천62명을 대상으로 임상시험을 진행한 결과 렘데시비르가 회복 기간을 5일 단축해줬다고 밝힌 바 있다\nx :  [67799]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-36b569856b90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m '''\n\u001b[1;32m     17\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mheadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-6b0519d23181>\u001b[0m in \u001b[0;36mheadline\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x : '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# valid_x = x + (article_max_len - len(x)) * [word_dict[\"<padding>\"]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mvalid_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marticle_max_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<padding>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid_x : '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-6b0519d23181>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x : '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# valid_x = x + (article_max_len - len(x)) * [word_dict[\"<padding>\"]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mvalid_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marticle_max_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<padding>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid_x : '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "txt = '''\n",
    "미국 제약회사 길리어드사이언스의 항바이러스제 '렘데시비르'가 신종 코로나바이러스 감염증(코로나19) 환자에게 미치는 효과가 거의 없다는 세계보건기구(WHO)의 연구 결과가 나왔다.\n",
    "\n",
    "WHO가 입원 환자 1만1천266명을 상대로 진행하고 있는 '연대 실험'에서 렘데시비르가 환자의 입원 기간을 줄이거나 사망률을 낮추지 못했다고 로이터통신이 15일 보도했다.\n",
    "\n",
    "WHO의 연대 실험은 코로나19 치료제 후보군의 효능과 안전성을 검증하기 위한 다국적 임상시험이다. 렘데시비르 외에 말라리아 치료제 하이드록시클로로퀸, 인간면역결핍바이러스(HIV) 치료제인 로피나비르/리토나비르, 항바이러스제 인터페론 등을 대상으로 진행되고 있다.\n",
    "\n",
    "시험 결과 이 약물 중 어떤 것도 실질적으로 사망률에 영향을 주거나 인공호흡기 사용 필요성을 줄여주지 못한 것으로 나타났다. 시험에 사용된 약물들은 렘데시비르와 하이드록시클로로퀸, 로피나비르, 인터페론 등이다. 또 이들 약물은 환자들의 병원 입원 기간에도 거의 영향을 주지 않았다.\n",
    "\n",
    "길리어드사이언스는 이달 초 코로나19 입원 환자 1천62명을 대상으로 임상시험을 진행한 결과 렘데시비르가 회복 기간을 5일 단축해줬다고 밝힌 바 있다.\n",
    "\n",
    "최근 코로나에 걸렸다가 완치 판정을 받은 도널드 트럼프 미국 대통령도 이 약을 투약했다.\n",
    "\n",
    "한편, 한국에서는 지난 13일까지 62개 병원에서 600명의 환자에게 렘데시비르를 투여했다고 방역 당국이 밝힌 바 있다.\n",
    "\n",
    "'''\n",
    "tf.reset_default_graph()\n",
    "headline(summary(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}